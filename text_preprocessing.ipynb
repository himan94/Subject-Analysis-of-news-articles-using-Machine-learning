{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing  \n",
    "\n",
    "#### 1.1 Reading the files\n",
    "\n",
    "We begin by reading all the articles training and testing into an appropriate data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open(\"training_testing_combined.txt\",'r',encoding = 'utf8') as fn:\n",
    "    content = fn.read().lower()\n",
    "\n",
    "#Extracting all the articles and their names from the file\n",
    "articles = re.findall(r'id t[re]_doc_[0-9]+[\\s]text(.*?)eod', content, flags = re.S )\n",
    "article_names = re.findall(r'id (t[re]_doc_[0-9]+)', content, flags = re.S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Finding Bigrams and generating tokens\n",
    "\n",
    "We now try to find 100 most meaningful bigrams using nltk library. We would then use these bigrams to tokenise our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-_]\\w*)?\")\n",
    "\n",
    "unigram_articles = []\n",
    "for article in articles:\n",
    "    #tokenise the articles\n",
    "    tokens = tokenizer.tokenize(article)\n",
    "    unigram_articles.append(tokens)\n",
    "    \n",
    "#generating list of tokens to find meaningful bigrams    \n",
    "words = list(chain.from_iterable(tokenized_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.collocations\n",
    "#Removing possible number tokens from words list prior to finding bigrams\n",
    "words = [word for word in words if (word.isdigit() == False and len(word) > 1) ]\n",
    "#Finding possible bigrams\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "\n",
    "with open('stopwords_en.txt','r') as stopwords:\n",
    "    stopword = stopwords.readlines()\n",
    "    \n",
    "stopword = [word.rstrip() for word in stopword_content] \n",
    "\n",
    "#Applying filters to bigrams to get rid of any stop words in collocation and bigrams with low frequency as they carry little information\n",
    "finder.apply_word_filter(lambda word: word in stopword)\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "#Applying measuring technique to identify first 200 collocations (meaningful bigrams)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_vocab = list(finder.nbest(bigram_measures.likelihood_ratio, 100))\n",
    "print(\"100 Most commonly appearing bigrams\\n\",bigram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_vocab = [('south', 'wales'), ('prime', 'minister'), ('united', 'states'), ('chief', 'executive'), ('federal', 'government'), ('world', 'number'), ('years', 'ago'), ('abc', 'news'), ('opposition', 'leader'), ('state', 'government'), ('australian', 'open'), ('north', 'queensland'), ('western', 'australia'), ('south', 'australia'), ('fire', 'service'), ('minister', 'john'), ('south', 'australian'), ('local', 'time'), ('weeks', 'ago'), ('friday', 'night'), ('saturday', 'night'), ('central', 'coast'), ('western', 'australian'), ('health', 'minister'), ('news', 'abc'), ('home', 'side'), ('months', 'ago'), ('city', 'council'), ('central', 'queensland'), ('federal', 'opposition'), ('long', 'time'), ('country', 'fire'), ('north', 'coast'), ('local', 'government'), ('western', \"australia's\"), ('south', 'coast'), ('recent', 'years'), ('australian', 'government'), ('told', 'abc'), ('world', 'record'), ('south', \"australia's\"), ('wales', 'government'), ('high', 'court'), ('killed', 'people'), ('health', 'department'), ('good', 'news'), ('federal', 'police'), ('lost', 'control'), ('federal', 'court'), ('match', 'point'), ('health', 'services'), ('public', 'health'), ('melbourne', 'victory'), ('chief', 'minister'), ('days', 'ago'), ('minister', 'peter'), ('million', 'people'), ('yesterday', 'morning'), ('queensland', 'premier'), ('recent', 'weeks'), ('coach', 'john'), ('queensland', 'government'), ('queensland', 'health'), ('west', 'coast'), ('western', 'sydney'), ('south', 'sydney'), ('services', 'minister'), ('hard', 'work'), ('west', 'australian'), ('centre', 'court'), ('sunday', 'morning'), ('health', 'service'), ('premier', 'peter'), ('man', 'died'), ('saturday', 'morning'), ('wales', 'premier'), ('hit', 'back'), ('year', 'ago'), ('coming', 'back'), ('open', 'final'), ('central', 'west'), ('earlier', 'today'), ('put', 'forward'), ('taking', 'part'), ('court', 'today'), ('australian', 'federal'), ('sunday', 'night'), ('working', 'hard'), ('people', 'died'), ('recent', 'months'), ('premier', 'john'), ('police', 'chief'), ('early', 'hours'), ('late', 'yesterday'), ('coming', 'days'), ('people', 'including'), ('week', 'ago'), ('make', 'decision'), ('past', 'year'), ('past', 'years')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use multiword tokeniser to tokenise documents into unigrams and bigrams.\n",
    "\n",
    "#### 1.3 Finding unwanted tokens\n",
    "\n",
    "We now count the document frequency of each token to determine if they will be cosidered into our vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "doc_freq = {}\n",
    "\n",
    "tokenizer = nltk.tokenize.mwe.MWETokenizer(bigram_vocab, separator = '_')\n",
    "\n",
    "tokenized_articles = []\n",
    "for doc_number in range(len(unigram_articles)):\n",
    "    #tokenise the articles\n",
    "    token_article = tokenizer.tokenize(unigram_articles[doc_number])\n",
    "    tokenized_articles.append(token_article)\n",
    "    unique_tokens = set(token_article)\n",
    "    \n",
    "    #count occurence of each token in number of articles\n",
    "    for token in unique_tokens:\n",
    "        if token in doc_freq:\n",
    "            doc_freq[token] = doc_freq[token] + 1\n",
    "        else:\n",
    "            doc_freq[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining tokens occuring in 95% and 3000 of the documents and that are of length less than 3\n",
    "tokens_95_threshold = []            \n",
    "tokens_rare_token = []\n",
    "token_len_3 = []\n",
    "\n",
    "no_of_docs = len(articles)\n",
    "\n",
    "#tokens with document frequency greater than 95% and less than 3000\n",
    "for token,count in doc_freq.items():\n",
    "    if count > (.95 * no_of_docs):\n",
    "        tokens_95_threshold.append(token)\n",
    "    elif count < 3000:\n",
    "        tokens_rare_token.append(token)\n",
    "\n",
    "#tokens of lenght less than 3\n",
    "for token in doc_freq.keys():\n",
    "    if len(token) < 3:\n",
    "        token_len_3.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining a list of stopwords for english\n",
    "with open('stopwords_en.txt','r') as stopwords:\n",
    "    stopword_content = stopwords.readlines()\n",
    "    \n",
    "stopword_content = [word.rstrip() for word in stopword_content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tokens that are occurring in more than 95% of the documents, are of length less than 2 and have a document frequence less than 3000 are considered as unwanted and we would remove them from our vocab/feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all the tokens that needs to be removed including stop words\n",
    "for rare_token in tokens_rare_token:\n",
    "    stopword_content.append(rare_token)\n",
    "    \n",
    "for token in token_len_3:\n",
    "    stopword_content.append(token)\n",
    "    \n",
    "for token in tokens_95_threshold:\n",
    "    stopword_content.append(token)\n",
    "\n",
    "#a set of all the unrequired tokens\n",
    "stopword_content_set = set(stopword_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 tagging, lemmatizing and stemming\n",
    "\n",
    "In order to get a better vocab/feature set we first tag all the tokens. This is followed by lemmatization which reduces the words to its lexical base form. Once lemmatised we remove all the stop words from our vocab and stem the words. Stemming helps in reducing two similar words to root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "lemmatized_articles = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for article in tokenized_articles:\n",
    "    tagged_article =  nltk.tag.pos_tag(article)\n",
    "    stopped_tagged_article = [word for word in tagged_article if word[0] not in stopword_content_set]\n",
    "    lemmat_article = [lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) for w in stopped_tagged_article]\n",
    "    lemmatized_articles.append(lemmat_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_articles = []\n",
    "for article in lemmatized_articles:\n",
    "    stem_article = [stemmer.stem(token) for token in article]\n",
    "    stemmed_articles.append(stem_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 vectorising and generation of TD-IF matirx\n",
    "\n",
    "After obtaining stemmed tokens and removing unwanted tokens we now procceed to generate a TD-IF matrix for all the tokens and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining clean articles in text format for vectorization\n",
    "text_articles = [' '.join(article) for article in stemmed_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Initialising vectorizer\n",
    "vectorizer = TfidfVectorizer(input = 'content',analyzer = 'word', token_pattern = r\"\\w+(?:[-_]\\w*)?\" )\n",
    "\n",
    "#Obtaining tf-ids for each term in the articles\n",
    "article_vector = vectorizer.fit_transform(text_articles).todense()\n",
    "\n",
    "#obtaining vectorised term frequency of each article in a dataframe\n",
    "articles_transformed = pd.DataFrame(article_vector, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Generating csv with article lables\n",
    "\n",
    "We now convert the generated TD-IDF matirix into a csv file. We split the data into test and traininig files and update training data with lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding article names to the vectorised article dataframe\n",
    "article_names = pd.Series(article_names)\n",
    "articles_transformed[\"article_names\"] = article_names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and test data\n",
    "training_data = articles_transformed.iloc[0:106445,]\n",
    "test_data = articles_transformed.iloc[106445:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining article lables\n",
    "lable_file = open(\"training_labels_final.txt\",'r')\n",
    "article_lable = {}\n",
    "for line in lable_file:\n",
    "    match = re.match(r'([\\w_]+)\\s([C]\\w+)',line)\n",
    "    article_lable[match.group(1)] = match.group(2)\n",
    "lable_file.close()\n",
    "\n",
    "#function to map lables\n",
    "def lable_map(article_name):\n",
    "    global article_lable\n",
    "    \n",
    "    return article_lable[article_name]\n",
    "\n",
    "#Updating tranformed articles data frame with article lables\n",
    "training_data[\"article_lable\"] = training_data[\"article_names\"].apply(lable_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tranformed vectorised articles in a csv for further analysis\n",
    "training_data.to_csv('training_data1.csv', sep=',', encoding='utf-8', index = False)\n",
    "test_data.to_csv('test_data1.csv', sep=',', encoding='utf-8', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
